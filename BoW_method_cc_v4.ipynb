{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Kindly Note : if import Fails PIP install corresponding library\n",
    "import json\n",
    "import requests\n",
    "import sklearn\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "import gensim \n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "import numpy\n",
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FindAlternateGroups_v4(url):\n",
    "    def GetTitlesLinks(url_link):\n",
    "            #Set the product collection common URL string\n",
    "            text1 = 'collections/all/products.json?page='\n",
    "            #Initialize lists of Titles and Links to products for final JSON array\n",
    "            all_titles = []\n",
    "            all_links = []\n",
    "            #We assume there are MAX 50 pages of products to iterate over each page, this value can be changed\n",
    "            for i in range(1,50):\n",
    "                #Strng for Final URL to each product\n",
    "                final_url = url_link + text1 + str(i)\n",
    "                print(final_url)\n",
    "                #Request JSON data\n",
    "                response = requests.get(final_url)\n",
    "                #Load JSON into variable\n",
    "                response_json = json.loads(response.text)\n",
    "                #Select PRODUCTS from JSON for further analysis\n",
    "                products_json = response_json['products']\n",
    "                #If PRODUCTS not empty\n",
    "                if products_json:\n",
    "                    #Iterate products to get their title and links\n",
    "                    for pattern in products_json:\n",
    "                        title = pattern['title']\n",
    "                        links = pattern['handle']\n",
    "                        all_titles.append(title)\n",
    "                        all_links.append(links)\n",
    "                else:\n",
    "                    break\n",
    "            #print(all_titles)\n",
    "            #print(all_links)\n",
    "            return all_titles, all_links\n",
    "    \n",
    "    def clean_sentence(sentence, stopwords=False):\n",
    "        #Perform text pre-processing and cleaning for Bag of Words\n",
    "        sentence = sentence.lower().strip()  #Convert to lower case\n",
    "        sentence = re.sub(r'[^a-z0-9\\s]', '', sentence) #Remove special chars\n",
    "        \n",
    "        if stopwords:\n",
    "            #The Gensim pre-built function to remove all the stop words if set TRUE\n",
    "            sentence = remove_stopwords(sentence)\n",
    "        return sentence\n",
    "                        \n",
    "    def get_cleaned_sentences(df,stopwords=False):    \n",
    "        #Empty an array of cleaned Titles\n",
    "        cleaned_sentences=[]\n",
    "        #Iterate to get cleaned titles\n",
    "        for index in range(len(df)):\n",
    "            #print(index,row)\n",
    "            cleaned=clean_sentence(df[index],stopwords)\n",
    "            cleaned_sentences.append(cleaned)\n",
    "        return cleaned_sentences\n",
    "    \n",
    "    def retrieveSimilarProducts(question, question_embedded,sentence_embeddings,sentences, max_sim):\n",
    "        #Receives the title, title Embedding vectors, original title string and max_sim to filterout Alternate/Similar products\n",
    "        index_sim=-1\n",
    "        #Dict for storing individual product alternates\n",
    "        dict1 = {}\n",
    "        var = question\n",
    "        var_2 = url_link + \"/products/\"\n",
    "        #List to store links to Laternate products\n",
    "        li = []\n",
    "        #Interate over each title and get similarity index, store if has it exceeds max_sim\n",
    "        for index,pattern_embedding in enumerate(sentence_embeddings):\n",
    "            sim=cosine_similarity(pattern_embedding,question_embedded)[0][0];\n",
    "            #print(index, sim, sentences[index])\n",
    "            if (sim>max_sim):\n",
    "                li.append(var_2+links[index])\n",
    "        dict1.update({var:li})        \n",
    "        #print(li)\n",
    "        #print(dict1)\n",
    "        return dict1\n",
    "    \n",
    "    url_link = url\n",
    "    titles, links = GetTitlesLinks(url_link)\n",
    "    cleaned_sentences_with_stopwords=get_cleaned_sentences(titles,stopwords=False)\n",
    "    #print(cleaned_sentences_with_stopwords)\n",
    "    sentences=cleaned_sentences_with_stopwords\n",
    "    # The next piece of code splits the sentences by white spaces \n",
    "    sentence_words = [[word for word in document.split() ]\n",
    "            for document in sentences]\n",
    "    #Create a new Corpus Dictionary for all the words\n",
    "    dictionary = corpora.Dictionary(sentence_words)\n",
    "    bow_corpus = [dictionary.doc2bow(text) for text in sentence_words]\n",
    "    #List to hold the final JSON for output\n",
    "    dict_main=[]\n",
    "    #Final iteration, clubbing everything\n",
    "    for i in range(len(titles)):\n",
    "        a = clean_sentence(titles[i],stopwords=False)\n",
    "        a = dictionary.doc2bow(a.split())\n",
    "        c = retrieveSimilarProducts(titles[i],a, bow_corpus, titles, max_sim=0.95)\n",
    "        dict_main.append(c)\n",
    "    print (dict_main)\n",
    "    #output to a JSON file\n",
    "    with open(\"output_v2.json\", \"w\") as outfile:\n",
    "        json.dump(dict_main, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Execute this cell last. Change the string of text to the desired URL and code will do the need for you.\n",
    "\"\"\"\n",
    "text = \"https://www.woolsboutiqueuomo.com/\"\n",
    "FindAlternateGroups_v4(url=text)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
